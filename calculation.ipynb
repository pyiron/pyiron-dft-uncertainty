{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style(style='white') \n",
    "sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T20:04:32.783206Z",
     "start_time": "2020-12-03T20:04:31.642028Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "from scipy import linalg\n",
    "import pandas\n",
    "import time\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T20:04:33.700749Z",
     "start_time": "2020-12-03T20:04:32.784336Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyiron_atomistics import Project\n",
    "from pyiron_dft_uncertainty import (\n",
    "    update_uncertainty_parameter,\n",
    "    get_alat_range,\n",
    "    calc_set_of_jobs,\n",
    "    setup_pyiron_table,\n",
    "    get_potential_encut,\n",
    "    get_column_from_df,\n",
    "    calc_v0_from_fit_funct,\n",
    "    bulk_modulus_from_fit_and_volume,\n",
    "    bulk_modulus_dereivative_from_fit_and_volume,\n",
    "    double_smooth,\n",
    "    shift_lst,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', np.RankWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_parameter = {\n",
    "    'element': config[\"element_settings\"]['element'],\n",
    "    'pseudo_potential': config[\"element_settings\"]['pseudo_potential'],\n",
    "    'crystal_structure': config[\"element_settings\"]['crystal_structure'],\n",
    "    'alat_guess': config[\"element_settings\"]['alat_guess'],\n",
    "    'encut_start': config[\"convergence_settings\"]['encut_start'],\n",
    "    'encut_step': config[\"convergence_settings\"]['encut_step'],\n",
    "    'encut_end': config[\"convergence_settings\"]['encut_end'],\n",
    "    'kpoint_start': config[\"convergence_settings\"]['kpoint_start'],\n",
    "    'kpoint_step': config[\"convergence_settings\"]['kpoint_step'],\n",
    "    'kpoint_end': config[\"convergence_settings\"]['kpoint_end'],\n",
    "    'points': config[\"evcurve_settings\"]['points'],\n",
    "    'vol_range': config[\"evcurve_settings\"]['vol_range'],\n",
    "    'points_pre': config[\"evcurve_settings\"]['points_pre'],\n",
    "    'vol_range_pre': config[\"evcurve_settings\"]['vol_range_pre'],\n",
    "    'queue': config[\"server_settings\"]['queue'],\n",
    "    'cores': config[\"server_settings\"]['cores'], \n",
    "    'run_time': config[\"server_settings\"]['run_time'], \n",
    "    'memory_factor': config[\"server_settings\"]['memory_factor'],\n",
    "    'project_name': config[\"server_settings\"]['project_name'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_period=30\n",
    "iteration=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill up empty parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_parameter = update_uncertainty_parameter(\n",
    "    uncertainty_parameter=uncertainty_parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(name=\"pyiron/data/\" + uncertainty_parameter[\"element\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T20:04:34.333340Z",
     "start_time": "2020-12-03T20:04:34.328351Z"
    }
   },
   "outputs": [],
   "source": [
    "encut_space = np.arange(\n",
    "    uncertainty_parameter['encut_start'], \n",
    "    uncertainty_parameter['encut_end'] + uncertainty_parameter['encut_step'], \n",
    "    uncertainty_parameter['encut_step']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T20:04:34.343245Z",
     "start_time": "2020-12-03T20:04:34.334269Z"
    }
   },
   "outputs": [],
   "source": [
    "kpoint_space = np.arange(\n",
    "    uncertainty_parameter['kpoint_start'], \n",
    "    uncertainty_parameter['kpoint_end'] + uncertainty_parameter['kpoint_step'], \n",
    "    uncertainty_parameter['kpoint_step']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T20:04:34.351012Z",
     "start_time": "2020-12-03T20:04:34.344261Z"
    }
   },
   "outputs": [],
   "source": [
    "cores = uncertainty_parameter['cores'] # compute cores  \n",
    "memory_factor = uncertainty_parameter['memory_factor'] # x-times 3GB memory \n",
    "queue = uncertainty_parameter['queue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_pre = Project(uncertainty_parameter['project_name'] + \"_pre\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_base = pr_pre.create.structure.ase.bulk(uncertainty_parameter[\"element\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alat_ten_space = get_alat_range(\n",
    "    vol_eq=structure_base.get_volume(), \n",
    "    strain=uncertainty_parameter['vol_range_pre'], \n",
    "    steps=uncertainty_parameter['points_pre'], \n",
    "    crystal_structure=uncertainty_parameter['crystal_structure'], \n",
    "    number_of_atoms=len(structure_base)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_lst = []\n",
    "for alat in alat_ten_space:\n",
    "    parameter_lst.append([alat, encut_space[-1], kpoint_space[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_name_pre = \"pyiron/data/\" + uncertainty_parameter[\"element\"] + \"/\" + uncertainty_parameter['project_name'].lower() + \"_pre.csv\"\n",
    "if not os.path.exists(df_file_name_pre):\n",
    "    calc_set_of_jobs(\n",
    "        pr=pr_pre, \n",
    "        parameter_lst=parameter_lst, \n",
    "        uncertainty_parameter=uncertainty_parameter, \n",
    "        vasp_parameter=config[\"vasp_settings\"],\n",
    "        sleep_period=sleep_period, \n",
    "        iteration=iteration\n",
    "    )\n",
    "    wait_for_jobs_to_be_done(\n",
    "        project=pr_pre, \n",
    "        sleep_period=sleep_period, \n",
    "        iteration=iteration\n",
    "    )\n",
    "    pytab = setup_pyiron_table(project=pr_pre)\n",
    "    df_tmp = pytab.get_dataframe()\n",
    "    df_tmp.to_csv(df_file_name_pre)\n",
    "calc_df_pre = pandas.read_csv(df_file_name_pre, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_lst, eng_lst = read_from_df(df=calc_df_pre, encut=encut_space[-1], kpoint=kpoint_space[-1], vol_space=vol_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_pre = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_pre_lst, eng_pre_lst = zip(*[[vol, eng] for vol, eng in zip(vol_lst, eng_lst) if eng is not None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_pre = np.poly1d(np.polyfit(vol_pre_lst, eng_pre_lst, degree_pre))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_range_pre_big = np.linspace(\n",
    "    (1-float(uncertainty_parameter['vol_range_pre']))*vol_eq_pre, \n",
    "    (1+float(uncertainty_parameter['vol_range_pre']))*vol_eq_pre, \n",
    "    int(uncertainty_parameter['points_pre']) * 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vol_pre_lst, eng_pre_lst, \"x\")\n",
    "plt.plot(vol_range_pre_big, fit_pre(vol_range_pre_big))\n",
    "plt.xlabel(\"Volume\")\n",
    "plt.ylabel(\"Energy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_eq = np.round(calc_v0_from_fit_funct(fit_funct=fit_pre, x=vol_pre_lst), 4)\n",
    "vol_eq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = Project(uncertainty_parameter['project_name']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T20:04:34.407240Z",
     "start_time": "2020-12-03T20:04:34.400060Z"
    }
   },
   "outputs": [],
   "source": [
    "alat_space = get_alat_range(\n",
    "    vol_eq=vol_eq, \n",
    "    strain=uncertainty_parameter['vol_range'], \n",
    "    steps=uncertainty_parameter['points'], \n",
    "    crystal_structure=uncertainty_parameter['crystal_structure'], \n",
    "    number_of_atoms=len(structure_base)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_lst = []\n",
    "for kpoints in kpoint_space:\n",
    "    for encut in encut_space:\n",
    "        for alat in alat_space:\n",
    "            parameter_lst.append([alat, encut, kpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-21T06:23:35.976Z"
    }
   },
   "outputs": [],
   "source": [
    "df_file_name = \"pyiron/data/\" + uncertainty_parameter[\"element\"] + \"/\" + uncertainty_parameter['project_name'].lower() + \".csv\"\n",
    "if not os.path.exists(df_file_name):\n",
    "    calc_set_of_jobs(\n",
    "        pr=pr, \n",
    "        parameter_lst=parameter_lst, \n",
    "        uncertainty_parameter=uncertainty_parameter,\n",
    "        vasp_parameter=config[\"vasp_settings\"],\n",
    "        sleep_period=sleep_period, \n",
    "        iteration=iteration\n",
    "    )\n",
    "    wait_for_jobs_to_be_done(\n",
    "        project=pr, \n",
    "        sleep_period=sleep_period, \n",
    "        iteration=iteration\n",
    "    )\n",
    "    pytab = setup_pyiron_table(project=pr)\n",
    "    df_tmp = pytab.get_dataframe()\n",
    "    df_tmp.to_csv(df_file_name)\n",
    "calc_df = pandas.read_csv(df_file_name, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encut_recommended = get_potential_encut(\n",
    "    el=uncertainty_parameter['pseudo_potential'].split(\"_\")[0], \n",
    "    default_potential=uncertainty_parameter['pseudo_potential']\n",
    ")\n",
    "ind_select = encut_recommended < encut_space\n",
    "encut_min = np.min(np.arange(len(encut_space))[ind_select])\n",
    "encut_space[encut_min], encut_recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_space = [\n",
    "    pr.create_ase_bulk(element, a=alat).get_volume() \n",
    "    for alat in alat_space\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_store_lst = []\n",
    "for encut in encut_space:\n",
    "    for kpoint in kpoint_space:\n",
    "        vol_lst, eng_lst, conv_lst, n_kpts_lst, avg_plane_waves_lst = zip(*[\n",
    "            [\n",
    "                get_column_from_df(df=calc_df, column='volume', kpoint=kpoint, encut=encut, volume=vol_name, volume_digits=6), \n",
    "                get_column_from_df(df=calc_df, column='energy_tot', kpoint=kpoint, encut=encut, volume=vol_name, volume_digits=6),\n",
    "                get_column_from_df(df=calc_df, column='el_conv', kpoint=kpoint, encut=encut, volume=vol_name, volume_digits=6),\n",
    "                get_column_from_df(df=calc_df, column='n_equ_kpts', kpoint=kpoint, encut=encut, volume=vol_name, volume_digits=6),\n",
    "                get_column_from_df(df=calc_df, column='avg. plane waves', kpoint=kpoint, encut=encut, volume=vol_name, volume_digits=6)\n",
    "            ] for vol_name in vol_space])\n",
    "        vol_lst, eng_lst, conv_lst, n_kpts_lst, avg_plane_waves_lst = np.array(vol_lst), np.array(eng_lst), np.array(conv_lst), np.array(n_kpts_lst), np.array(avg_plane_waves_lst)\n",
    "        eng_store_lst.append(eng_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_store_mat = np.array(eng_store_lst, dtype=float).reshape(\n",
    "    len(encut_space), \n",
    "    len(kpoint_space), \n",
    "    len(vol_space)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_store_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systematic Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpoint_min_tmp = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = cm.get_cmap('Spectral').reversed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, 3, figsize=(15, 5), dpi=80)\n",
    "vlen = len(encut_space)\n",
    "for i, encut in enumerate(encut_space[encut_min:]):\n",
    "    ediff_low = np.array(eng_store_mat[-1][kpoint_min]) - np.array(eng_store_mat[encut_min+i][kpoint_min])\n",
    "    ediff_high = np.array(eng_store_mat[-1][-1]) - np.array(eng_store_mat[encut_min+i][-1])\n",
    "    axs[0].plot(vol_space, ediff_low * 1000, \".-\", color=cmap((i+encut_min)/vlen))\n",
    "    axs[0].plot(vol_space, ediff_high * 1000, \".--\", color=cmap((i+encut_min)/vlen))\n",
    "vlen = len(kpoint_space)\n",
    "for j, kpoint in enumerate(kpoint_space[kpoint_min:]):\n",
    "    ediff_low = np.array(eng_store_mat[encut_min][-1]) - np.array(eng_store_mat[encut_min][kpoint_min+j])\n",
    "    ediff_high = np.array(eng_store_mat[-1][-1]) - np.array(eng_store_mat[-1][kpoint_min+j])\n",
    "    axs[1].plot(vol_space, ediff_low * 1000, \".-\",color=cmap((j+kpoint_min)/vlen))\n",
    "    axs[1].plot(vol_space, ediff_high * 1000, \".--\", color=cmap((j+kpoint_min)/vlen))\n",
    "axs[0].set_xlabel(\"Volume\")\n",
    "axs[0].set_ylabel(\"$\\Delta$ Noise (meV)\")\n",
    "axs[1].set_xlabel(\"Volume\")\n",
    "axs[1].set_ylabel(\"$\\Delta$ Noise (meV)\")\n",
    "normalize = mcolors.Normalize(vmin=encut_space.min(), vmax=encut_space.max())\n",
    "scalarmappaple = cm.ScalarMappable(norm=normalize, cmap=cmap)\n",
    "scalarmappaple.set_array(encut_space)\n",
    "cbar = f.colorbar(scalarmappaple, ax=axs[0])\n",
    "cbar.set_label(\"Encut\")\n",
    "normalize = mcolors.Normalize(vmin=kpoint_space.min(), vmax=kpoint_space.max())\n",
    "scalarmappaple = cm.ScalarMappable(norm=normalize, cmap=cmap)\n",
    "scalarmappaple.set_array(kpoint_space)\n",
    "cbar = f.colorbar(scalarmappaple, ax=axs[1])\n",
    "cbar.set_label(\"Kpoint Mesh\")\n",
    "\n",
    "vlen = len(encut_space)\n",
    "for i, encut in enumerate(encut_space[encut_min:]):\n",
    "    std_lst = []\n",
    "    for j, kpoint in enumerate(kpoint_space[kpoint_min_tmp:]):\n",
    "        ediff = np.array(eng_store_mat)[i+encut_min][-1]-np.array(eng_store_mat)[i+encut_min][j+kpoint_min_tmp]-(np.array(eng_store_mat)[-1][-1]-np.array(eng_store_mat)[-1][j+kpoint_min_tmp])\n",
    "        std_lst.append(np.std([v for v in ediff if not np.isnan(v)]))\n",
    "    axs[2].plot(kpoint_space[kpoint_min_tmp:], std_lst, color=cmap((i+encut_min)/vlen))\n",
    "    axs[2].plot(kpoint_space[kpoint_min_tmp:], shift_lst(std_lst), \"--\", color=cmap((i+encut_min)/vlen))\n",
    "axs[2].set_xlabel(\"Kpoint Mesh\")\n",
    "axs[2].set_ylabel(\"$\\sigma(\\Delta$ Noise)\")\n",
    "axs[2].set_yscale(\"log\")\n",
    "axs[2].set_xscale(\"log\")\n",
    "normalize = mcolors.Normalize(vmin=encut_space.min(), vmax=encut_space.max())\n",
    "scalarmappaple = cm.ScalarMappable(norm=normalize, cmap=cmap)\n",
    "scalarmappaple.set_array(encut_space)\n",
    "cbar = f.colorbar(scalarmappaple, ax=axs[2])\n",
    "cbar.set_label(\"Encut ($\\epsilon$)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, 3, figsize=(15, 5), dpi=80)\n",
    "\n",
    "vlen = len(kpoint_space)\n",
    "for i, encut in enumerate(encut_space):\n",
    "    std_lst = []\n",
    "    for j, kpoint in enumerate(kpoint_space):\n",
    "        ediff = np.array(eng_store_mat)[i][-1]-np.array(eng_store_mat)[i][j]-(np.array(eng_store_mat)[-1][-1]-np.array(eng_store_mat)[-1][j])\n",
    "        std_lst.append(np.std([v for v in ediff if not np.isnan(v)]))\n",
    "    axs[0].plot(kpoint_space, std_lst, color=cmap(i/vlen))\n",
    "    axs[0].plot(kpoint_space, shift_lst(std_lst), \"--\", color=cmap(i/vlen))\n",
    "axs[0].set_xlabel(\"Kpoint Mesh\")\n",
    "axs[0].set_ylabel(\"$\\sigma(\\Delta$ Noise)\")\n",
    "axs[0].set_yscale(\"log\")\n",
    "normalize = mcolors.Normalize(vmin=encut_space.min(), vmax=encut_space.max())\n",
    "scalarmappaple = cm.ScalarMappable(norm=normalize, cmap=cmap)\n",
    "scalarmappaple.set_array(encut_space)\n",
    "cbar = f.colorbar(scalarmappaple, ax=axs[0])\n",
    "cbar.set_label(\"Encut ($\\epsilon$)\")\n",
    "\n",
    "vlen = len(encut_space)\n",
    "for j, kpoint in enumerate(kpoint_space):\n",
    "    std_lst = []\n",
    "    for i, encut in enumerate(encut_space):\n",
    "        ediff = np.array(eng_store_mat)[-1][j]-np.array(eng_store_mat)[i][j]-(np.array(eng_store_mat)[-1][-1]-np.array(eng_store_mat)[i][-1])\n",
    "        std_lst.append(np.std([v for v in ediff if not np.isnan(v)]))\n",
    "    axs[1].plot(encut_space, std_lst, color=cmap(j/vlen))\n",
    "    axs[1].plot(encut_space, shift_lst(std_lst), \"--\", color=cmap(j/vlen))\n",
    "axs[1].set_xlabel(\"EnCut\")\n",
    "axs[1].set_ylabel(\"$\\sigma(\\Delta$ Noise)\")\n",
    "axs[1].set_yscale(\"log\")\n",
    "normalize = mcolors.Normalize(vmin=kpoint_space.min(), vmax=kpoint_space.max())\n",
    "scalarmappaple = cm.ScalarMappable(norm=normalize, cmap=cmap)\n",
    "scalarmappaple.set_array(kpoint_space)\n",
    "cbar = f.colorbar(scalarmappaple, ax=axs[1])\n",
    "cbar.set_label(\"Kpoint Mesh ($\\kappa_{x}, \\kappa_{y}, \\kappa_{z}$)\")\n",
    "\n",
    "degree = 13\n",
    "fit_base = np.poly1d(np.polyfit(vol_space, eng_store_mat[-1][-1], degree))\n",
    "geo_range = np.geomspace(10 ** -7, 10 ** -2, 60)\n",
    "b0_noise_lst = []\n",
    "for ediff_tmp in geo_range:\n",
    "    b0_tmp_lst = []\n",
    "    for i in range(100):\n",
    "        eng_test = fit_base(vol_space) + np.random.normal(loc=0.0, scale=ediff_tmp, size=len(vol_space))\n",
    "        fit_tmp = np.poly1d(np.polyfit(vol_space, eng_test, degree))\n",
    "        v0_tmp = calc_v0_from_fit_funct(fit_funct=fit_tmp, x=vol_space)\n",
    "        b0_tmp = bulk_modulus_from_fit_and_volume(fit_funct=fit_tmp, v_0=v0_tmp, convert_to_gpa=True)\n",
    "        b0_tmp_lst.append(b0_tmp)\n",
    "    b0_noise_lst.append(np.std(np.array(b0_tmp_lst)[~np.isnan(b0_tmp_lst)]))\n",
    "fit_b0_noise = np.polyfit(np.log(geo_range[~np.isnan(b0_noise_lst)]), np.log(np.array(b0_noise_lst)[~np.isnan(b0_noise_lst)]), 1)\n",
    "y_calc = np.exp(np.poly1d(fit_b0_noise)(np.log(geo_range)))\n",
    "\n",
    "\n",
    "axs[2].plot(geo_range, b0_noise_lst, \"-\", color=\"C0\")\n",
    "axs[2].plot(geo_range, y_calc, \"--\", color=\"C0\")\n",
    "axs[2].set_xlabel(\"$\\sigma(\\Delta Noise)$\")\n",
    "axs[2].set_ylabel(\"$\\Delta$ B0\")\n",
    "axs[2].set_yscale(\"log\")\n",
    "axs[2].set_xscale(\"log\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0_level = 1\n",
    "e0_level = np.max(geo_range[y_calc<b0_level])\n",
    "e0_level, np.sqrt(e0_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_max_max_lst, eng_max_max_noise_lst = [], []\n",
    "for i, encut in enumerate(encut_space):\n",
    "    for j, kpoint in enumerate(kpoint_space):\n",
    "        eng = np.array(eng_store_mat[-1][j]) - np.array(eng_store_mat[i][j]) - np.array(eng_store_mat[-1][-1]) + np.array(eng_store_mat[i][-1])\n",
    "        eng_max_max_noise_lst.append(np.std([en for en in eng if not np.isnan(en)]))\n",
    "        eng = eng_store_mat[-1][-1]-(eng_store_mat[-1][-1]-np.array(eng_store_mat)[-1][j])-(eng_store_mat[-1][-1]-np.array(eng_store_mat)[i][-1])\n",
    "        eng_max_max_lst.append(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_max_max_mat = np.array(eng_max_max_lst, dtype=float).reshape(len(encut_space), len(kpoint_space), len(vol_space))\n",
    "eng_max_max_noise_mat = np.array(eng_max_max_noise_lst, dtype=float).reshape(len(encut_space), len(kpoint_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0_max_lst, b0_max_lst, bp_max_lst = [], [], []\n",
    "v0_max_std_lst, b0_max_std_lst, bp_max_std_lst = [], [], []\n",
    "for e, eng_lst, noise_lst in zip(encut_space, eng_max_max_mat, eng_max_max_noise_mat):\n",
    "    for k, eng, noise_std in zip(kpoint_space, eng_lst, noise_lst):\n",
    "        vol_lst, eng_lst = np.array([[vo, en] for vo, en in zip(vol_space, eng) if not np.isnan(en)]).T\n",
    "        fit = np.poly1d(np.polyfit(vol_lst, eng_lst, degree))\n",
    "        v0_tmp = calc_v0_from_fit_funct(fit_funct=fit, x=vol_space)\n",
    "        b0_tmp = bulk_modulus_from_fit_and_volume(fit_funct=fit, v_0=v0_tmp, convert_to_gpa=True)\n",
    "        bp_tmp = bulk_modulus_dereivative_from_fit_and_volume(fit_funct=fit, v_0=v0_tmp)\n",
    "        v0_max_lst.append(v0_tmp)\n",
    "        b0_max_lst.append(b0_tmp)\n",
    "        bp_max_lst.append(bp_tmp)\n",
    "        v0_tmp_lst, b0_tmp_lst, bp_tmp_lst = [], [], []\n",
    "        for i in range(100):\n",
    "            eng_test = fit(vol_space) + np.random.normal(loc=0.0, scale=noise_std, size=len(vol_space))\n",
    "            fit_tmp = np.poly1d(np.polyfit(vol_space, eng_test, degree))\n",
    "            v0_tmp = calc_v0_from_fit_funct(fit_funct=fit_tmp, x=vol_space)\n",
    "            b0_tmp = bulk_modulus_from_fit_and_volume(fit_funct=fit_tmp, v_0=v0_tmp, convert_to_gpa=True)\n",
    "            bp_tmp = bulk_modulus_dereivative_from_fit_and_volume(fit_funct=fit_tmp, v_0=v0_tmp)\n",
    "            v0_tmp_lst.append(v0_tmp)\n",
    "            b0_tmp_lst.append(b0_tmp)\n",
    "            bp_tmp_lst.append(bp_tmp)\n",
    "        v0_max_std_lst.append(np.std(v0_tmp_lst))\n",
    "        b0_max_std_lst.append(np.std(b0_tmp_lst))\n",
    "        bp_max_std_lst.append(np.std(bp_tmp_lst))\n",
    "v0_max_mat = np.array(v0_max_lst).reshape(len(encut_space), len(kpoint_space))\n",
    "b0_max_mat = np.array(b0_max_lst).reshape(len(encut_space), len(kpoint_space))\n",
    "bp_max_mat = np.array(bp_max_lst).reshape(len(encut_space), len(kpoint_space))\n",
    "v0_max_std_mat = np.array(v0_max_std_lst).reshape(len(encut_space), len(kpoint_space))\n",
    "b0_max_std_mat = np.array(b0_max_std_lst).reshape(len(encut_space), len(kpoint_space))\n",
    "bp_max_std_mat = np.array(bp_max_std_lst).reshape(len(encut_space), len(kpoint_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0_raw_lst, b0_raw_lst, bp_raw_lst = [], [], []\n",
    "for e, eng_lst in zip(encut_space, eng_store_mat):\n",
    "    for k, eng in zip(kpoint_space, eng_lst):\n",
    "        vol_lst, eng_lst = np.array([[vo, en] for vo, en in zip(vol_space, eng) if not np.isnan(en)]).T\n",
    "        fit = np.poly1d(np.polyfit(vol_lst, eng_lst, degree))\n",
    "        v0_tmp = calc_v0_from_fit_funct(fit_funct=fit, x=vol_space)\n",
    "        b0_tmp = bulk_modulus_from_fit_and_volume(fit_funct=fit, v_0=v0_tmp, convert_to_gpa=True)\n",
    "        bp_tmp = bulk_modulus_dereivative_from_fit_and_volume(fit_funct=fit, v_0=v0_tmp)\n",
    "        v0_raw_lst.append(v0_tmp)\n",
    "        b0_raw_lst.append(b0_tmp)\n",
    "        bp_raw_lst.append(bp_tmp)\n",
    "v0_raw_mat = np.array(v0_raw_lst).reshape(len(encut_space), len(kpoint_space))\n",
    "b0_raw_mat = np.array(b0_raw_lst).reshape(len(encut_space), len(kpoint_space))\n",
    "bp_raw_mat = np.array(bp_raw_lst).reshape(len(encut_space), len(kpoint_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0_max_diff_mat = double_smooth(np.abs(b0_max_mat-b0_max_mat[-1,-1]))\n",
    "b0_raw_diff_mat = double_smooth(np.abs(b0_raw_mat-b0_raw_mat[-1,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpoint_mesh, encut_mesh = np.meshgrid(kpoint_space, encut_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = np.geomspace(10 ** -4, 10 ** 2, 7)\n",
    "\n",
    "f, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "for n, ax in enumerate(axs.flatten()):\n",
    "    ax.text(-0.1, 1.06, string.ascii_lowercase[n] + \")\", transform=ax.transAxes, \n",
    "            size=16)\n",
    "\n",
    "axs[0].set_title(\"$\\Delta B_{0}(\\epsilon, \\kappa)$ max - noise\")\n",
    "mappable = axs[0].contourf(kpoint_mesh, encut_mesh, double_smooth(b0_max_std_mat), vmin=np.min(levels), vmax=np.max(levels), levels=levels, norm=mcolors.LogNorm(), cmap=\"viridis\")\n",
    "bar = f.colorbar(mappable, ax=axs[0])\n",
    "cbar.set_label(\"$\\Delta B_{0} (GPa)$\")\n",
    "axs[0].set_xlabel(\"Kpoint Mesh ($\\kappa_{x}, \\kappa_{y}, \\kappa_{z}$)\")\n",
    "axs[0].set_ylabel(\"EnCut ($\\epsilon$ in eV)\")\n",
    "\n",
    "axs[1].set_title(\"$\\Delta B_{0}(\\epsilon, \\kappa)$ max - systematic\")\n",
    "mappable = axs[1].contourf(kpoint_mesh, encut_mesh, b0_max_diff_mat, vmin=np.min(levels), vmax=np.max(levels), levels=levels, norm=mcolors.LogNorm(), cmap=\"viridis\")\n",
    "bar = f.colorbar(mappable, ax=axs[1])\n",
    "cbar.set_label(\"$\\Delta B_{0} (GPa)$\")\n",
    "axs[1].set_xlabel(\"Kpoint Mesh ($\\kappa_{x}, \\kappa_{y}, \\kappa_{z}$)\")\n",
    "axs[1].set_ylabel(\"EnCut ($\\epsilon$ in eV)\")\n",
    "\n",
    "axs[2].set_title(\"$\\Delta B_{0}(\\epsilon, \\kappa)$ max - combined\")\n",
    "mappable = axs[2].contourf(kpoint_mesh, encut_mesh, double_smooth(b0_max_diff_mat + b0_max_std_mat), vmin=np.min(levels), vmax=np.max(levels), levels=levels, norm=mcolors.LogNorm(), cmap=\"viridis\")\n",
    "bar = f.colorbar(mappable, ax=axs[2])\n",
    "cbar.set_label(\"$\\Delta B_{0} (GPa)$\")\n",
    "axs[2].set_xlabel(\"Kpoint Mesh ($\\kappa_{x}, \\kappa_{y}, \\kappa_{z}$)\")\n",
    "axs[2].set_ylabel(\"EnCut ($\\epsilon$ in eV)\")\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
